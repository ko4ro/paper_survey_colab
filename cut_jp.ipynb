{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cut_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ko4ro/paper_survey_colab/blob/main/cut_jp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSwHyBbaejXe"
      },
      "source": [
        "# Contrastive Unpaired Translation (CUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv6A0IPlfkf-"
      },
      "source": [
        "<td><a target=\"_blank\" href=\"https://github.com/taesungp/contrastive-unpaired-translation\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub</a></td>\n",
        "  <td>     <a href=\"http://taesung.me/ContrastiveUnpairedTranslation/\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">元論文をダウンロード</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pQVj7X0gOsH"
      },
      "source": [
        "このノートブックでは、「[Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/pdf/2007.1565)」について、実際にコードを動かしながら、内容を理解していくものである。\n",
        "\n",
        "この論文では教師なし画像変換タスクにおいて [Contrastive(対照)学習](https://qiita.com/omiita/items/a7429ec42e4eef4b6a4d)を有効に使う方法を示している。ポイントとして、本研究では画像全体ではなく、パッチ単位かつ多層でContrastive(対照)学習を行っている。\n",
        "またパッチのNegativeサンプルは他の画像から得るのではなく、入力画像内からサンプリングすることでパッチ間の相互情報量が最大化されることを期待しています。従来手法と比較し性能を向上していることに加え、学習時間も短縮できることを実証している。さらには、それぞれの「ドメイン」が1枚の画像のみである場合においても、学習できるように拡張することができる。\n",
        "\n",
        "![cut_horse2zebra](https://raw.githubusercontent.com/ko4ro/paper_survey_colab/main/asset/figures/cut_horse2zebra.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbLQyVl0Wu2q"
      },
      "source": [
        "## Gitのリポジトリをクローンしてくる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk97ZsRa0oMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14eea478-233d-4580-fa69-42934ed06207"
      },
      "source": [
        "!git clone  -b feature/colab https://github.com/ko4ro/contrastive-unpaired-translation.git cut\n",
        "!pip install -r ./cut/requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cut' already exists and is not an empty directory.\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ./cut/requirements.txt (line 1)) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r ./cut/requirements.txt (line 2)) (0.10.0+cu102)\n",
            "Requirement already satisfied: dominate>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ./cut/requirements.txt (line 3)) (2.6.0)\n",
            "Requirement already satisfied: visdom>=0.1.8.8 in /usr/local/lib/python3.7/dist-packages (from -r ./cut/requirements.txt (line 4)) (0.1.8.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from -r ./cut/requirements.txt (line 5)) (21.0)\n",
            "Requirement already satisfied: GPUtil>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ./cut/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r ./cut/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r ./cut/requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r ./cut/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (0.1.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (1.32)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (22.2.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->-r ./cut/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r ./cut/requirements.txt (line 4)) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3NDL5iiW176"
      },
      "source": [
        "## データセット準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm-vSMapknVQ"
      },
      "source": [
        "dataset_name = \"grumpifycat\" #@param [\"ae_photos\",\"apple2orange\", \"summer2winter_yosemite\", \"horse2zebra\", \"monet2photo\", \"cezanne2photo\", \"ukiyoe2photo\", \"vangogh2photo\", \"maps\", \"cityscapes\", \"facades\", \"iphone2dslr_flower\", \"mini\", \"mini_pix2pix\", \"mini_colorization\", \"grumpifycat\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tEgUGhylLa8",
        "outputId": "d5ea75d6-072f-4eac-bb21-c6034a9f4e59"
      },
      "source": [
        "!cd ./cut/ && bash datasets/download_cut_dataset.sh $dataset_name"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ FILE=grumpifycat\n",
            "+ [[ grumpifycat != \\a\\e\\_\\p\\h\\o\\t\\o\\s ]]\n",
            "+ [[ grumpifycat != \\a\\p\\p\\l\\e\\2\\o\\r\\a\\n\\g\\e ]]\n",
            "+ [[ grumpifycat != \\s\\u\\m\\m\\e\\r\\2\\w\\i\\n\\t\\e\\r\\_\\y\\o\\s\\e\\m\\i\\t\\e ]]\n",
            "+ [[ grumpifycat != \\h\\o\\r\\s\\e\\2\\z\\e\\b\\r\\a ]]\n",
            "+ [[ grumpifycat != \\m\\o\\n\\e\\t\\2\\p\\h\\o\\t\\o ]]\n",
            "+ [[ grumpifycat != \\c\\e\\z\\a\\n\\n\\e\\2\\p\\h\\o\\t\\o ]]\n",
            "+ [[ grumpifycat != \\u\\k\\i\\y\\o\\e\\2\\p\\h\\o\\t\\o ]]\n",
            "+ [[ grumpifycat != \\v\\a\\n\\g\\o\\g\\h\\2\\p\\h\\o\\t\\o ]]\n",
            "+ [[ grumpifycat != \\m\\a\\p\\s ]]\n",
            "+ [[ grumpifycat != \\c\\i\\t\\y\\s\\c\\a\\p\\e\\s ]]\n",
            "+ [[ grumpifycat != \\f\\a\\c\\a\\d\\e\\s ]]\n",
            "+ [[ grumpifycat != \\i\\p\\h\\o\\n\\e\\2\\d\\s\\l\\r\\_\\f\\l\\o\\w\\e\\r ]]\n",
            "+ [[ grumpifycat != \\m\\i\\n\\i ]]\n",
            "+ [[ grumpifycat != \\m\\i\\n\\i\\_\\p\\i\\x\\2\\p\\i\\x ]]\n",
            "+ [[ grumpifycat != \\m\\i\\n\\i\\_\\c\\o\\l\\o\\r\\i\\z\\a\\t\\i\\o\\n ]]\n",
            "+ [[ grumpifycat != \\g\\r\\u\\m\\p\\i\\f\\y\\c\\a\\t ]]\n",
            "+ [[ grumpifycat == \\c\\i\\t\\y\\s\\c\\a\\p\\e\\s ]]\n",
            "+ echo 'Specified [grumpifycat]'\n",
            "Specified [grumpifycat]\n",
            "+ URL=https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/grumpifycat.zip\n",
            "+ ZIP_FILE=./datasets/grumpifycat.zip\n",
            "+ TARGET_DIR=./datasets/grumpifycat/\n",
            "+ wget --no-check-certificate -N https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/grumpifycat.zip -O ./datasets/grumpifycat.zip\n",
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2021-08-18 14:11:17--  https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/grumpifycat.zip\n",
            "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20399194 (19M) [application/zip]\n",
            "Saving to: ‘./datasets/grumpifycat.zip’\n",
            "\n",
            "./datasets/grumpify 100%[===================>]  19.45M  2.51MB/s    in 6.3s    \n",
            "\n",
            "2021-08-18 14:11:24 (3.10 MB/s) - ‘./datasets/grumpifycat.zip’ saved [20399194/20399194]\n",
            "\n",
            "+ mkdir ./datasets/grumpifycat/\n",
            "mkdir: cannot create directory ‘./datasets/grumpifycat/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUZ8_VFvW7qt"
      },
      "source": [
        "## インポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJRgJGvIeBUW",
        "outputId": "b0b29389-06db-4a04-8666-7633dfb6948f"
      },
      "source": [
        "import sys\n",
        "ROOT_PATH = '/content/cut/'\n",
        "sys.path.append(ROOT_PATH)\n",
        "print(sys.path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/cut/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGeU4lRxYnAy"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from options.train_options import TrainOptions\n",
        "from data import create_dataset\n",
        "from models import create_model\n",
        "from util.visualizer import Visualizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEcb-lCRXBQN"
      },
      "source": [
        "## 学習パラメータの準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qonIQcoRqD-F",
        "outputId": "9a5f3047-db03-4723-bab1-ba85e6faf1e5"
      },
      "source": [
        "opt = TrainOptions().parse()  # get training options"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- Options ---------------\n",
            "                 CUT_mode: CUT                           \n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "                    beta2: 0.999                         \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "           continue_train: False                         \n",
            "                crop_size: 256                           \n",
            "                 dataroot: placeholder                   \n",
            "             dataset_mode: unaligned                     \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: None                          \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "               easy_label: experiment_name               \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "          evaluation_freq: 5000                          \n",
            "        flip_equivariance: False                         \n",
            "                 gan_mode: lsgan                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: xavier                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "               lambda_GAN: 1.0                           \n",
            "               lambda_NCE: 1.0                           \n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: cut                           \n",
            "                 n_epochs: 200                           \n",
            "           n_epochs_decay: 200                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: experiment_name               \n",
            "                    nce_T: 0.07                          \n",
            "                  nce_idt: True                          \n",
            "nce_includes_all_negatives_from_minibatch: False                         \n",
            "               nce_layers: 0,4,8,12,16                   \n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netF: mlp_sample                    \n",
            "                  netF_nc: 256                           \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "             no_antialias: False                         \n",
            "          no_antialias_up: False                         \n",
            "               no_dropout: True                          \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                    normD: instance                      \n",
            "                    normG: instance                      \n",
            "              num_patches: 256                           \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 0                             \n",
            "               preprocess: resize_and_crop               \n",
            "          pretrained_name: None                          \n",
            "               print_freq: 100                           \n",
            "         random_scale_max: 3.0                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "stylegan2_G_num_downsampling: 1                             \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFeGMP9Sc1eL"
      },
      "source": [
        "opt.dataroot = f'/content/cut/datasets/{dataset_name}'\n",
        "opt.name = f'{dataset_name}_CUT'\n",
        "opt.n_epochs = 5\n",
        "opt.n_epochs_decay = 5"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p3PmfhOZfOy",
        "outputId": "f9d1cde1-6059-425c-838e-e71e932bc825"
      },
      "source": [
        "dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
        "dataset_size = len(dataset)    # get the number of images in the dataset."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset [UnalignedDataset] was created\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_M5GeuSdlnI",
        "outputId": "e7a83ab6-7674-4c28-ec23-6d4e3be294c7"
      },
      "source": [
        "model = create_model(opt)      # create a model given opt.model and other options\n",
        "print('The number of training images = %d' % dataset_size)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model [CUTModel] was created\n",
            "The number of training images = 214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OioKWcP3duv7",
        "outputId": "46fa0221-33fe-4242-f574-79def8c8e545"
      },
      "source": [
        "visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up a new session...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "create web directory ./checkpoints/grumpifycat_CUT/web...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpS01uOqdy7R"
      },
      "source": [
        "opt.visualizer = visualizer\n",
        "total_iters = 0\n",
        "optimize_time = 0.1\n",
        "times = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4fyiXzWd79H",
        "outputId": "17e79100-3f76-4a0e-e339-473494117752"
      },
      "source": [
        "for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n",
        "        epoch_start_time = time.time()  # timer for entire epoch\n",
        "        iter_data_time = time.time()    # timer for data loading per iteration\n",
        "        epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
        "        visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n",
        "\n",
        "        dataset.set_epoch(epoch)\n",
        "        for i, data in enumerate(dataset):  # inner loop within one epoch\n",
        "            iter_start_time = time.time()  # timer for computation per iteration\n",
        "            if total_iters % opt.print_freq == 0:\n",
        "                t_data = iter_start_time - iter_data_time\n",
        "\n",
        "            batch_size = data[\"A\"].size(0)\n",
        "            total_iters += batch_size\n",
        "            epoch_iter += batch_size\n",
        "            if len(opt.gpu_ids) > 0:\n",
        "                torch.cuda.synchronize()\n",
        "            optimize_start_time = time.time()\n",
        "            if epoch == opt.epoch_count and i == 0:\n",
        "                model.data_dependent_initialize(data)\n",
        "                model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
        "                model.parallelize()\n",
        "            model.set_input(data)  # unpack data from dataset and apply preprocessing\n",
        "            model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
        "            if len(opt.gpu_ids) > 0:\n",
        "                torch.cuda.synchronize()\n",
        "            optimize_time = (time.time() - optimize_start_time) / batch_size * 0.005 + 0.995 * optimize_time\n",
        "\n",
        "            if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file\n",
        "                save_result = total_iters % opt.update_html_freq == 0\n",
        "                model.compute_visuals()\n",
        "                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n",
        "\n",
        "            if total_iters % opt.print_freq == 0:    # print training losses and save logging information to the disk\n",
        "                losses = model.get_current_losses()\n",
        "                visualizer.print_current_losses(epoch, epoch_iter, losses, optimize_time, t_data)\n",
        "                if opt.display_id is None or opt.display_id > 0:\n",
        "                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n",
        "\n",
        "            if total_iters % opt.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations\n",
        "                print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n",
        "                print(opt.name)  # it's useful to occasionally show the experiment name on console\n",
        "                save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'\n",
        "                model.save_networks(save_suffix)\n",
        "\n",
        "            iter_data_time = time.time()\n",
        "\n",
        "        if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n",
        "            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n",
        "            model.save_networks('latest')\n",
        "            model.save_networks(epoch)\n",
        "\n",
        "        print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))\n",
        "        model.update_learning_rate()                     # update learning rates at the end of every epoch."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------- Networks initialized -------------\n",
            "[Network G] Total number of parameters : 11.378 M\n",
            "[Network F] Total number of parameters : 0.560 M\n",
            "[Network D] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "(epoch: 1, iters: 100, time: 0.494, data: 0.229) G_GAN: 0.295 D_real: 0.197 D_fake: 0.238 G: 3.924 NCE: 3.682 NCE_Y: 3.575 \n",
            "(epoch: 1, iters: 200, time: 0.697, data: 0.002) G_GAN: 0.282 D_real: 0.271 D_fake: 0.196 G: 3.159 NCE: 2.979 NCE_Y: 2.775 \n",
            "End of epoch 1 / 10 \t Time Taken: 228 sec\n",
            "learning rate = 0.0002000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(epoch: 2, iters: 86, time: 0.819, data: 0.005) G_GAN: 0.361 D_real: 0.137 D_fake: 0.283 G: 2.806 NCE: 2.578 NCE_Y: 2.312 \n",
            "(epoch: 2, iters: 186, time: 0.894, data: 0.002) G_GAN: 0.348 D_real: 0.297 D_fake: 0.147 G: 2.515 NCE: 2.268 NCE_Y: 2.066 \n",
            "End of epoch 2 / 10 \t Time Taken: 217 sec\n",
            "learning rate = 0.0002000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(epoch: 3, iters: 72, time: 0.939, data: 0.002) G_GAN: 0.407 D_real: 0.101 D_fake: 0.195 G: 2.200 NCE: 1.867 NCE_Y: 1.720 \n",
            "(epoch: 3, iters: 172, time: 0.966, data: 0.002) G_GAN: 0.498 D_real: 0.262 D_fake: 0.097 G: 2.003 NCE: 1.502 NCE_Y: 1.507 \n",
            "End of epoch 3 / 10 \t Time Taken: 216 sec\n",
            "learning rate = 0.0002000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(epoch: 4, iters: 58, time: 0.983, data: 0.002) G_GAN: 0.510 D_real: 0.122 D_fake: 0.158 G: 1.936 NCE: 1.437 NCE_Y: 1.414 \n",
            "(epoch: 4, iters: 158, time: 0.993, data: 0.002) G_GAN: 0.206 D_real: 0.270 D_fake: 0.302 G: 1.868 NCE: 1.501 NCE_Y: 1.823 \n",
            "End of epoch 4 / 10 \t Time Taken: 217 sec\n",
            "learning rate = 0.0002000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(epoch: 5, iters: 44, time: 0.999, data: 0.003) G_GAN: 0.311 D_real: 0.146 D_fake: 0.348 G: 1.677 NCE: 1.369 NCE_Y: 1.364 \n",
            "(epoch: 5, iters: 144, time: 1.003, data: 0.002) G_GAN: 0.518 D_real: 0.248 D_fake: 0.153 G: 1.976 NCE: 1.560 NCE_Y: 1.356 \n",
            "saving the model at the end of epoch 5, iters 1070\n",
            "End of epoch 5 / 10 \t Time Taken: 217 sec\n",
            "learning rate = 0.0001667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(epoch: 6, iters: 30, time: 1.005, data: 0.002) G_GAN: 0.430 D_real: 0.011 D_fake: 0.463 G: 3.011 NCE: 2.526 NCE_Y: 2.636 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}